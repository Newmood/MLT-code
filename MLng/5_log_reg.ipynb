{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab451bf5",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a classification algorithm, whose model is as follows:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    z & = \\vec{w} \\cdot \\vec{x} + b \\\\\n",
    "    f_{\\vec{w},b}(\\vec{x}) &= \\dfrac{1}{1+e^{-z}}, \\quad 0< f_{\\vec{w},b}(\\vec{x})<1\n",
    "\\end{align*}\n",
    "$$\n",
    "The output, $f_{\\vec{w},b} (\\vec{x})$ represents the probability for the input $\\vec{x}$ with parameters $\\vec{w},b$ is class 1. That is,\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = P(y=1|\\vec{x};(\\vec{w},b)) $$\n",
    "\n",
    "- We check if $f_{\\vec{w},b}(\\vec{x}) \\geq 0.5$, if yes then we predict, $\\hat{y}=1$\n",
    "else we say, $\\hat{y}=0$  \n",
    "- **Decision boundary equation is:** $\\boxed{\\vec{w}\\cdot \\vec{x} + b = 0}$\n",
    "\n",
    "---\n",
    "## Cost function for logistic regression\n",
    "$$\n",
    "\\begin{align*}\n",
    "    J(\\vec{w},b) = \\dfrac{1}{n} \\left[\\sum_{i=1}^{n} -y^{(i)}log(f_{\\vec{w},b} (\\vec{x}^{(i)})) - (1-y^{(i)})log(1-f_{\\vec{w},b}(\\vec{x}^{(i)})) \\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb57bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8e1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e64c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_reg(X,y,w,b):\n",
    "    \"\"\"\n",
    "    Computes cost for logistic regression\n",
    "    \n",
    "    Arguments:\n",
    "    X : (ndarray (n,d)), Data matrix, n observation as rows and d features as rows\n",
    "    y : (ndarray (n,)), Target values vector \n",
    "    w : (ndarray (d,)), Parameter, d-dimensional, one value for each feature\n",
    "    b : scalar, parameter\n",
    "    Outputs:\n",
    "    cost : (scalar) Cost for the logistic regression model\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    cost = 0.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        z_i = np.dot(X[i],w) +b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        \n",
    "        cost = cost + -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "        \n",
    "    cost = cost/n\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02ebdd",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression\n",
    "There not much difference in gradient descent algorithm than in previous modules except $f_{\\vec{w},b}(\\vec{x}) $ is different:\n",
    "$$\n",
    "    w_1 \\leftarrow w_1 - \\alpha \\dfrac{1}{n}\\sum_{i=1}^{n} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})(x_1^{(i)}) \\\\\n",
    "    \\vdots\\\\\n",
    "    w_d \\leftarrow w_d - \\alpha \\dfrac{1}{n}\\sum_{i=1}^{n} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})(x_d^{(i)})\n",
    "$$\n",
    "and $$b \\leftarrow b - \\alpha \\dfrac{1}{n}\\sum_{i=1}^{n} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d94a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdv_logreg(X,y,w,b):\n",
    "    \"\"\"\n",
    "    Computes the partial derivatives for gradient descent in logistic regression\n",
    "    \n",
    "    Arguments:\n",
    "    X : (ndarray (n,d)), Data matrix, n observation as rows and d features as rows\n",
    "    y : (ndarray (n,)), Target values vector \n",
    "    w : (ndarray (d,)), Parameter, d-dimensional, one value for each feature\n",
    "    b : scalar, parameter\n",
    "    \n",
    "    Output:\n",
    "    dj_dw : (ndarray(d,)), Gradient of cost function w.r.t w\n",
    "    dj-db : (scalar), Partial derivative of cost function w.r.t b\n",
    "    \"\"\"\n",
    "    \n",
    "    n,d = X.shape\n",
    "    dj_dw = np.zeros(d,)\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w)+b)\n",
    "        \n",
    "        for j in range(d):\n",
    "            dj_dw[j] = dj_dw[j] + (f_wb_i - y[i])*X[i,j]\n",
    "        dj_db = dj_db + (f_wb_i - y[i])\n",
    "    \n",
    "    dj_dw = dj_dw/n\n",
    "    dj_db = dj_db/n\n",
    "    \n",
    "    return dj_dw,dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb496df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_logreg(X, y, w_in, b_in, alpha, num):\n",
    "    \"\"\"\n",
    "    Computes gradient descent for logistic regression\n",
    "    \n",
    "    Arguments:\n",
    "    X : (ndarray (n,d)), Data matrix, n observation as rows and d features as rows\n",
    "    y : (ndarray (n,)), Target values vector \n",
    "    w_in : (ndarray (d,)), Parameter, d-dimensional, one value for each feature\n",
    "    b_in : scalar, parameter\n",
    "    alpha : learning rate\n",
    "    num : (int), number of iterations\n",
    "    \n",
    "    Outputs: \n",
    "    w : final value of parameter after gradient descent\n",
    "    b : final value of parameter after gradient descent\n",
    "    J_hist : (list), cost values list\n",
    "    \"\"\"\n",
    "    \n",
    "    J_hist = []\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num):\n",
    "        dj_dw,dj_db = pdv_logreg(X,y,w,b)\n",
    "        \n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db\n",
    "    \n",
    "        if i<10000:\n",
    "            J_hist.append(cost_log_reg(X,y,w,b))\n",
    "    \n",
    "    return w,b,J_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf5d5e",
   "metadata": {},
   "source": [
    "### Working with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef1a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate 5 data points for each class\n",
    "class_0 = np.random.normal(0, 1, (5, 2))  # Class 0 centered at (0,0)\n",
    "class_1 = np.random.normal(2, 1, (5, 2))  # Class 1 centered at (2,2)\n",
    "\n",
    "# Combine the data points\n",
    "X = np.vstack([class_0, class_1])\n",
    "y = np.hstack([np.zeros(5), np.ones(5)])  # Labels (0 for class 0, 1 for class 1)\n",
    "\n",
    "# Shuffle the data (since we stacked class_0 first and class_1 second)\n",
    "shuffle_idx = np.random.permutation(len(X))\n",
    "X = X[shuffle_idx]\n",
    "y = y[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ea9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in = np.random.randn(2)\n",
    "b_in = 0\n",
    "alpha =0.06\n",
    "num = 1000\n",
    "w,b, j_hist = grad_desc_logreg(X,y,w_in,b_in,alpha,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c003f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
